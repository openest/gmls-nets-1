{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "This example trains a GMLS convolutional neural network on the MNIST dataset. We apply convolution -> batch renormalization three times followed by dense layers. This architecture is similar to strided convolutional networks. Each convolution layer downsamples the point cloud while increasing the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gmlsnets_tensorflow as gnets\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads mnist data. Normalizes the image data. Computes one-hot encoding of labels\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = (1.0*train_images/255).astype('float32')\n",
    "test_images = (1.0*test_images/255).astype('float32')\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pixels per side\n",
    "n=28\n",
    "\n",
    "# channel widths\n",
    "chans1=8\n",
    "chans2=16\n",
    "chans3=32\n",
    "\n",
    "# constructs 2d Taylor basis of order 2\n",
    "order = 2\n",
    "dim = 2\n",
    "fP = gnets.bases.Taylor(dim,order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point clouds and epsilon balls to compute neighbors\n",
    "x1 = 1.0*n*np.reshape(list(itertools.product(np.linspace(0,1,n),np.linspace(0,1,n))),\n",
    "            (n**2,2)).astype('float32')\n",
    "eps1 = (1.0*n/n)*(order+0.5)\n",
    "x2 = 1.0*n*np.reshape(list(itertools.product(np.linspace(0,1,n//2),np.linspace(0,1,n//2))),\n",
    "            ((n//2)**2,2)).astype('float32')\n",
    "eps2 = (2.0*n/n)*(order+0.5)\n",
    "x3 = 1.0*n*np.reshape(list(itertools.product(np.linspace(0,1,n//4),np.linspace(0,1,n//4))),\n",
    "            ((n//4)**2,2)).astype('float32')\n",
    "eps3 = (4.0*n/n)*(order+0.5)\n",
    "x4 = 1.0*n*np.reshape(list(itertools.product(np.linspace(0,1,n//8),np.linspace(0,1,n//8))),\n",
    "            ((n//8)**2,2)).astype('float32')\n",
    "eps4 = (8.0*n/n)*(order+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructs the model\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Reshape((n**2,1),input_shape=(n,n)))\n",
    "\n",
    "model.add(gnets.MFConvLayer(x1,x2,fP,gnets.weightfuncs.sixth,eps1,chans1,activation='elu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(-1))\n",
    "\n",
    "model.add(gnets.MFConvLayer(x2,x3,fP,gnets.weightfuncs.sixth,eps2,chans2,activation='elu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(-1))\n",
    "\n",
    "model.add(gnets.MFConvLayer(x3,x4,fP,gnets.weightfuncs.sixth,eps4,chans3,activation='elu'))\n",
    "model.add(tf.keras.layers.BatchNormalization(-1))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(100, activation='elu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgpatel/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/rgpatel/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/rgpatel/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/rgpatel/.local/lib/python3.5/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.8602 - accuracy: 0.7236\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s 157us/sample - loss: 0.3574 - accuracy: 0.8894\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 9s 157us/sample - loss: 0.2512 - accuracy: 0.9222\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 153us/sample - loss: 0.1940 - accuracy: 0.9389\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.1549 - accuracy: 0.9517\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.1276 - accuracy: 0.9595\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.1066 - accuracy: 0.9668\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.0912 - accuracy: 0.9719\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.0791 - accuracy: 0.9758\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.0695 - accuracy: 0.9789\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.0608 - accuracy: 0.9816\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.0543 - accuracy: 0.9836\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.0492 - accuracy: 0.9853\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 9s 148us/sample - loss: 0.0443 - accuracy: 0.9866\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 9s 149us/sample - loss: 0.0385 - accuracy: 0.9888\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 9s 149us/sample - loss: 0.0344 - accuracy: 0.9899\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 9s 149us/sample - loss: 0.0306 - accuracy: 0.9910\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 9s 149us/sample - loss: 0.0274 - accuracy: 0.9926\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.0243 - accuracy: 0.9936\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 9s 149us/sample - loss: 0.0220 - accuracy: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5a51312dd8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trains model\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          batch_size=501,\n",
    "          epochs=20,\n",
    "          verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "print('test set accuracy:',model.evaluate(test_images,test_labels, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
